# -*- coding: utf-8 -*-
"""Quantum_regression_vs_classical_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qPAwSoSk6c3u5un2CKIMYhh8uKjFfGUR
"""

pip install qiskit

pip install pylatexenc

pip install scikit-learn==1.1.3

# Commented out IPython magic to ensure Python compatibility.
import qiskit
from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister
from qiskit import Aer, execute
import pylatexenc
import math
import random
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
import sympy



# %matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
#imports datasets from scikit-learn
from sklearn import datasets
#loads Boston dataset from datasets library
from sklearn.datasets import load_boston
from sklearn import metrics

def inner_prod_norm_output(vec1, vec2):
    #first check lengths are equal
    if len(vec1) != len(vec2):
        raise ValueError('Lengths of states are not equal')
    N = len(vec1)
    nqubits = math.ceil(np.log2(N))    # compute how many qubits needed to encode either x or y

    xnorm = np.linalg.norm(vec1)           #sqrt(0^2+1^2+3^2+4^2+5^2+6^2+7^2)        # normalise vectors x and y
    ynorm = np.linalg.norm(vec2)
    vec1 = vec1/xnorm
    vec2= vec2/ynorm
    circ = QuantumCircuit(2*nqubits+1,1)
    circ.h(0)   # create circuit
    circ.initialize(vec1,qubits=range(1,nqubits+1))
    circ.initialize(vec2,range(nqubits+1,2*nqubits+1))
    for i in range(1,nqubits+1):
      circ.cswap(control_qubit=0,target_qubit1=i,target_qubit2=nqubits+i)
    circ.h(0)
    circ.measure(qubit=0,cbit=0)
    shut_number=1000
    job = execute(circ, backend=Aer.get_backend('qasm_simulator'),shots=shut_number)

    count = job.result().get_counts(circ)
    dot=np.sqrt(2*(count["0"]/shut_number)-1)  #2*p(0)-1   p(0)=number(0)/shut_number

    return dot

def anzats_7parameter_power2(x,parameters):
    y=parameters[0]*x[0]**2+parameters[1]*2*x[0]*x[1]+parameters[2]*2*x[0]*x[2]+parameters[3]*2*x[0]*x[3]
    +parameters[4]*2*x[0]*x[4]+parameters[5]*2*x[0]*x[5]+parameters[6]*2*x[0]*x[6]+parameters[7]*x[1]**2+parameters[8]*2*x[1]*x[2]
    +parameters[9]*2*x[1]*x[3]+parameters[10]*2*x[1]*x[4]+parameters[11]*2*x[1]*x[5]+parameters[12]*2*x[1]*x[6]
    +parameters[13]*x[2]**2+parameters[14]*2*x[2]*x[3]+parameters[15]*2*x[2]*x[4]+parameters[16]*2*x[2]*x[5]
    +parameters[17]*2*x[2]*x[6]+parameters[18]*x[3]**2+parameters[19]*2*x[3]*x[4]+parameters[20]*2*x[3]*x[5]
    +parameters[21]*2*x[3]*x[6]+parameters[22]*x[4]**2+parameters[23]*2*x[4]*x[5]+parameters[24]*2*x[4]*x[6]
    +parameters[25]*x[5]**2+parameters[26]*2*x[5]*x[6]+parameters[27]*x[6]**2
    return y



def anzats_5parameter_power1(x,parameters):
    y=parameters[0]*x[0]+parameters[1]*x[1]+parameters[2]*x[2]+parameters[3]*x[3]+parameters[4]*x[4]
    return y


def calculate_cost_function_7parameter_power2(parameters,x_all,y_all_truth):
    y_list=[]
    for x in x_all:

      y=anzats_7parameter_power2(x,parameters)


      y_list.append(y)


    y_Norm = np.linalg.norm(y_list)
    y_truth_norm  =np.linalg.norm(y_all_truth)   # normalise ansatz

    y_list = y_list/y_Norm
    y_all_truth=y_all_truth/ y_truth_norm

    y_ansatz = y_Norm /y_truth_norm  * inner_prod_norm_output(y_list, y_all_truth)     # use quantum circuit to test ansatz
                                                           # note the normalisation factors
    return (1-y_ansatz)**2


def calculate_cost_function_5parameter_power1(parameters,x_all,y_all_truth):
    y_list=[]
    for x in x_all:

      y=anzats_5parameter_power1(x,parameters)


      y_list.append(y)


    y_Norm = np.linalg.norm(y_list)
    y_truth_norm  =np.linalg.norm(y_all_truth)   # normalise ansatz

    y_list = y_list/y_Norm
    y_all_truth=y_all_truth/ y_truth_norm

    y_ansatz = y_Norm /y_truth_norm  * inner_prod_norm_output(y_list, y_all_truth)     # use quantum circuit to test ansatz
                                                           # note the normalisation factors
    return (1-y_ansatz)**2


def R_2_5parameter_power1(x_test,optimized_parameter,y_test):
  y_pred_list=[]
  for x,y in zip(x_test,y_test):
    y_pred=anzats_5parameter_power1(x,optimized_parameter)
    y_pred_list.append(y_pred)
  R2=metrics.r2_score(y_test, y_pred_list)
  return R2



x1,x2,x3,x4,x5,x6,x7 = sympy.symbols("x1 x2 x3 x4 x5 x6 x7")
formula = ((x1+x2+x3+x4+x5+x6+x7) **2).expand()
formula,(7**2+7)/2



x_all=[[1,2,3,4,8,5,6],[4,9,7,2,4,5,6],[7,8,1,0,7,8,9],[8,4,6,7,5,1,7]]
y_all=[4,5,9,8]
parameters=[random.uniform(0,2) for p in range(28)] #28=(7,2)+7
calculate_cost_function_7parameter_power2(parameters,x_all,y_all)

out = minimize(calculate_cost_function_power2, x0=parameters,args=(x_all,y_all), method="BFGS", options={'maxiter':200}, tol=1e-6)
out1 = minimize(calculate_cost_function_power2, x0=parameters,args=(x_all,y_all), method="COBYLA", options={'maxiter':200}, tol=1e-6)
out2 = minimize(calculate_cost_function_power2, x0=parameters, args=(x_all,y_all),method="Nelder-Mead", options={'maxiter':200}, tol=1e-6)
out3 = minimize(calculate_cost_function_power2, x0=parameters,args=(x_all,y_all), method="CG", options={'maxiter':200}, tol=1e-6)
out4 = minimize(calculate_cost_function_power2, x0=parameters,args=(x_all,y_all), method="trust-constr", options={'maxiter':200}, tol=1e-6)

x_all=[[1,2,3,4,8,5,6],[4,9,7,2,4,5,6],[7,8,1,0,7,8,9],[8,4,6,7,5,1,7]]
y_all=[4,5,9,8]

optimized_parameters_BFGS=out['x']
optimized_parameters_COBYLA=out1['x']
optimized_parameters_Nelder_Mead=out2['x']
optimized_parameters_CG=out3['x']
optimized_parameters_trust_constr=out4['x']

c_BFGS=calculate_cost_function_power2(optimized_parameters_BFGS,x_all,y_all)
c_COBYLA=calculate_cost_function_power2(optimized_parameters_COBYLA,x_all,y_all)
c_Nelder=calculate_cost_function_power2(optimized_parameters_Nelder_Mead,x_all,y_all)
c_CG=calculate_cost_function_power2(optimized_parameters_CG,x_all,y_all)
c_trust_constr=calculate_cost_function_power2(optimized_parameters_trust_constr,x_all,y_all)
c_BFGS,c_COBYLA,c_Nelder,c_CG,c_trust_constr



"""# compare Quanum and classical regression"""

def inner_prod_norm_output(vec1, vec2):
    #first check lengths are equal
    if len(vec1) != len(vec2):
        raise ValueError('Lengths of states are not equal')
    N = len(vec1)
    nqubits = math.ceil(np.log2(N))    # compute how many qubits needed to encode either x or y

    xnorm = np.linalg.norm(vec1)           #sqrt(0^2+1^2+3^2+4^2+5^2+6^2+7^2)        # normalise vectors x and y
    ynorm = np.linalg.norm(vec2)
    vec1 = vec1/xnorm
    vec2= vec2/ynorm
    circ = QuantumCircuit(2*nqubits+1,1)
    circ.h(0)   # create circuit
    circ.initialize(vec1,qubits=range(1,nqubits+1))
    circ.initialize(vec2,range(nqubits+1,2*nqubits+1))
    for i in range(1,nqubits+1):
      circ.cswap(control_qubit=0,target_qubit1=i,target_qubit2=nqubits+i)
    circ.h(0)
    circ.measure(qubit=0,cbit=0)
    shut_number=1000
    job = execute(circ, backend=Aer.get_backend('qasm_simulator'),shots=shut_number)

    count = job.result().get_counts(circ)
    dot=np.sqrt(2*(count["0"]/shut_number)-1)  #2*p(0)-1   p(0)=number(0)/shut_number

    return dot

def anzats_7parameter_power2(x,parameters):
    y=parameters[0]*x[0]**2+parameters[1]*2*x[0]*x[1]+parameters[2]*2*x[0]*x[2]+parameters[3]*2*x[0]*x[3]
    +parameters[4]*2*x[0]*x[4]+parameters[5]*2*x[0]*x[5]+parameters[6]*2*x[0]*x[6]+parameters[7]*x[1]**2+parameters[8]*2*x[1]*x[2]
    +parameters[9]*2*x[1]*x[3]+parameters[10]*2*x[1]*x[4]+parameters[11]*2*x[1]*x[5]+parameters[12]*2*x[1]*x[6]
    +parameters[13]*x[2]**2+parameters[14]*2*x[2]*x[3]+parameters[15]*2*x[2]*x[4]+parameters[16]*2*x[2]*x[5]
    +parameters[17]*2*x[2]*x[6]+parameters[18]*x[3]**2+parameters[19]*2*x[3]*x[4]+parameters[20]*2*x[3]*x[5]
    +parameters[21]*2*x[3]*x[6]+parameters[22]*x[4]**2+parameters[23]*2*x[4]*x[5]+parameters[24]*2*x[4]*x[6]
    +parameters[25]*x[5]**2+parameters[26]*2*x[5]*x[6]+parameters[27]*x[6]**2
    return y



def anzats_5parameter_power1(x,parameters):
    y=parameters[0]*x[0]+parameters[1]*x[1]+parameters[2]*x[2]+parameters[3]*x[3]+parameters[4]*x[4]
    return y


def calculate_cost_function_7parameter_power2(parameters,x_all,y_all_truth):
    y_list=[]
    for x in x_all:

      y=anzats_7parameter_power2(x,parameters)


      y_list.append(y)


    y_Norm = np.linalg.norm(y_list)
    y_truth_norm  =np.linalg.norm(y_all_truth)   # normalise ansatz

    y_list = y_list/y_Norm
    y_all_truth=y_all_truth/ y_truth_norm

    y_ansatz = y_Norm /y_truth_norm  * inner_prod_norm_output(y_list, y_all_truth)     # use quantum circuit to test ansatz
                                                           # note the normalisation factors
    return (1-y_ansatz)**2


def calculate_cost_function_5parameter_power1(parameters,x_all,y_all_truth):
    y_list=[]
    for x in x_all:

      y=anzats_5parameter_power1(x,parameters)


      y_list.append(y)


    y_Norm = np.linalg.norm(y_list)
    y_truth_norm  =np.linalg.norm(y_all_truth)   # normalise ansatz

    y_list = y_list/y_Norm
    y_all_truth=y_all_truth/ y_truth_norm

    y_ansatz = y_Norm /y_truth_norm  * inner_prod_norm_output(y_list, y_all_truth)     # use quantum circuit to test ansatz
                                                           # note the normalisation factors
    return (1-y_ansatz)**2


def R_2_5parameter_power1(x_test,optimized_parameter,y_test):
  y_pred_list=[]
  for x,y in zip(x_test,y_test):
    y_pred=anzats_5parameter_power1(x,optimized_parameter)
    y_pred_list.append(y_pred)
  R2=metrics.r2_score(y_test, y_pred_list)
  return R2

boston = load_boston()
# Initializing the dataframe
data = pd.DataFrame(boston.data, columns=boston.feature_names)
data['PRICE'] = boston.target
# Median value of owner-occupied

data.head()

# Finding out the correlation between the features
corr = data.corr()
print(corr.shape)
# Plotting the heatmap of correlation between features
plt.figure(figsize=(10,10))
sns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')

data.columns

X = data[[ 'LSTAT','RM', 'TAX','PTRATIO' ,'INDUS']]
#X = data["ZN"].values.reshape((-1,1))
y = data['PRICE']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 4)

# Splitting to training and testing data

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 4)

X_train=[list(X_train.values[i]) for i in range(len(X_train))][0:256]
X_test=[list(X_test.values[i]) for i in range(len(X_test))][0:128]

y_train=[y_train.values[i] for i in range(len(y_train))][0:256]
y_test=[y_test.values[i] for i in range(len(y_test))][0:128]

parameters=[random.uniform(0,2) for p in range(5)] #28=(7,2)+7
len(X_train),len(X_test)

"""# BFGS optimizer"""

out = minimize(calculate_cost_function_5parameter_power1, x0=parameters,args=(X_train,y_train), method="BFGS", options={'maxiter':200}, tol=1e-6)
optimized_parameters_BFGS=out['x']
print(optimized_parameters_BFGS)

R_2_5parameter_power1(X_test,optimized_parameters_BFGS,y_test)

"""# classical methode"""

# Import library for Linear Regression
from sklearn.linear_model import LinearRegression

# Create a Linear regressor
mlr = LinearRegression()

# Train the model using the training sets
mlr.fit(X_train, y_train)

X = data[[ 'LSTAT','RM', 'TAX','PTRATIO' ,'INDUS']]
#X = data["ZN"].values.reshape((-1,1))
y = data['PRICE']

# Splitting to training and testing data

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 4)

#Converting the coefficient values to a dataframe
coeffcients = pd.DataFrame([X_train.columns,mlr.coef_]).T
coeffcients = coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'})
coeffcients

# Model prediction on train data
y_pred = mlr.predict(X_test)

print('R^2:',metrics.r2_score(y_test, y_pred))

mlr.coef_

"""# COBYLA optimizer"""

X = data[[ 'LSTAT','RM', 'TAX','PTRATIO' ,'INDUS']]
#X = data["ZN"].values.reshape((-1,1))
y = data['PRICE']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 4)
X_train=[list(X_train.values[i]) for i in range(len(X_train))][0:256]
X_test=[list(X_test.values[i]) for i in range(len(X_test))][0:128]

y_train=[y_train.values[i] for i in range(len(y_train))][0:256]
y_test=[y_test.values[i] for i in range(len(y_test))][0:128]

parameters=[random.uniform(0,2) for p in range(5)] #28=(7,2)+7
len(X_train),len(X_test)

out1 = minimize(calculate_cost_function_5parameter_power1, x0=parameters,args=(X_train,y_train), method="COBYLA", options={'maxiter':200}, tol=1e-6)
optimized_parameters_COBYLA=out1['x']
print(optimized_parameters_COBYLA)
print(R_2_5parameter_power1(X_test,optimized_parameters_COBYLA,y_test))

"""## Nelder-Mead  **optimizer**"""

out2 = minimize(calculate_cost_function_5parameter_power1, x0=parameters,args=(X_train,y_train), method="Nelder-Mead", options={'maxiter':200}, tol=1e-6)
optimized_parameters_Nelder_Mead=out2['x']
print(optimized_parameters_Nelder_Mead)
print(R_2_5parameter_power1(X_test,optimized_parameters_Nelder_Mead,y_test))

"""## "CG"
"""

out3 = minimize(calculate_cost_function_5parameter_power1, x0=parameters,args=(X_train,y_train), method="CG", options={'maxiter':200}, tol=1e-6)
optimized_parameters_CG=out3['x']
print(optimized_parameters_CG)
print(R_2_5parameter_power1(X_test,optimized_parameters_CG,y_test))

"""# trust-constr"""

out4 = minimize(calculate_cost_function_5parameter_power1, x0=parameters,args=(X_train,y_train), method="trust-constr", options={'maxiter':200}, tol=1e-6)
optimized_parameters_trust_constr=out4['x']
print(optimized_parameters_trust_constr)
print(R_2_5parameter_power1(X_test,optimized_parameters_trust_constr,y_test))

optimized_parameters_BFGS=out['x']
optimized_parameters_COBYLA=out1['x']
optimized_parameters_Nelder_Mead=out2['x']
optimized_parameters_CG=out3['x']
optimized_parameters_trust_constr=out4['x']

# Import library for Linear Regression
from sklearn.linear_model import LinearRegression

# Create a Linear regressor
mlr = LinearRegression()

# Train the model using the training sets
mlr.fit(X_train, y_train)

#Converting the coefficient values to a dataframe
coeffcients = pd.DataFrame([X_train.columns,mlr.coef_]).T
coeffcients = coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'})
coeffcients

# Model prediction on train data
y_pred = mlr.predict(X_test)

# Model Evaluation
print('R^2:',metrics.r2_score(y_test, y_pred))
print('MAE:',metrics.mean_absolute_error(y_test, y_pred))
print('MSE:',metrics.mean_squared_error(y_test, y_pred))
print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

# Visualizing the differences between actual prices and predicted values
plt.scatter(y_test, y_pred)
plt.xlabel("Prices")
plt.ylabel("Predicted prices")
plt.title("Prices vs Predicted prices")
plt.show()



